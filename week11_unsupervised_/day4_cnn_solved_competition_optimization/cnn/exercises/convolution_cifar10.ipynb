{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\n",
    "\n",
    "A partir del dataset mnist (tf.keras.datasets.cifar10.load_data()), realiza una clasificación usando:\n",
    "\n",
    "** Las imágenes son a color, es decir, tienen 3 dimensiones (rgb). Para este problema, antes de empezar, transforma las imágenes a blanco y negro para que tengan 1 sola dimensión de profundidad **. \n",
    "\n",
    "1. Una CNN con:\n",
    "    - 1 capa convolutiva con 8 neuronas\n",
    "    - 1 MaxPool quedando las dimensiones de la imagen a la mitad\n",
    "    - 1 dropout 0.25\n",
    "    - 1 Flatten\n",
    "    - 1 dense con 32 neuronas\n",
    "    - 1 dense con 10 (salida)\n",
    "\n",
    "2. Una CNN con:\n",
    "    - 1 capa convolutiva con 8 neuronas\n",
    "    - 1 MaxPool quedando las dimensiones de la imagen a la mitad\n",
    "    - 1 dropout 0.25\n",
    "    - 1 Flatten\n",
    "    - 1 dense con 16 neuronas\n",
    "    - 1 dense con 32 neuronas\n",
    "    - 1 dense con 10 (salida)\n",
    "\n",
    "¿ Cuál ha dado mejor resultado?\n",
    "\n",
    "Para compilar el modelo, usa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "# TensorFlow y tf.keras\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow import keras\r\n",
    "from keras.layers import Dropout\r\n",
    "\r\n",
    "# Librerias de ayuda\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.compile(optimizer='adam',\r\n",
    " #             loss='sparse_categorical_crossentropy',\r\n",
    "  #            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = keras.datasets.cifar10\r\n",
    "#print(keras.datasets.cifar10.load_data())\r\n",
    "(train_images, train_labels), (test_images, test_labels) = data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train images shape: (50000, 32, 32, 3)\n",
      "train labes shape: (50000, 1)\n",
      "test images shape: (10000, 32, 32, 3)\n",
      "test labes shape: (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# data analysis\r\n",
    "print(f\"train images shape: {train_images.shape}\")\r\n",
    "print(f\"train labes shape: {train_labels.shape}\")\r\n",
    "\r\n",
    "print(f\"test images shape: {test_images.shape}\")\r\n",
    "print(f\"test labes shape: {test_labels.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from skimage.color import rgb2gray\r\n",
    "\r\n",
    "# transform 3D images to 2D\r\n",
    "train_images = rgb2gray(train_images)\r\n",
    "train_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\r\n",
    "plt.imshow(train_images[0], cmap=plt.cm.gray)\r\n",
    "plt.colorbar()\r\n",
    "plt.grid(False)\r\n",
    "\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Una CNN con:\r\n",
    "    - 1 capa convolutiva con 8 neuronas\r\n",
    "    - 1 MaxPool quedando las dimensiones de la imagen a la mitad\r\n",
    "    - 1 dropout 0.25\r\n",
    "    - 1 Flatten\r\n",
    "    - 1 dense con 32 neuronas\r\n",
    "    - 1 dense con 10 (salida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 30, 30, 8)         80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 15, 15, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_9 (Dropout)          (None, 15, 15, 8)         0         \n",
      "_________________________________________________________________\n",
      "flatten_9 (Flatten)          (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 32)                57632     \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 58,042\n",
      "Trainable params: 58,042\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model 1\r\n",
    "model = keras.Sequential([\r\n",
    "    keras.layers.Conv2D(filters=8, \r\n",
    "                        kernel_size=(3, 3), \r\n",
    "                        input_shape=(32, 32, 1), \r\n",
    "                        padding=\"valid\"),\r\n",
    "    keras.layers.MaxPooling2D(2, 2),\r\n",
    "    keras.layers.Dropout(0.25),\r\n",
    "    keras.layers.Flatten(),\r\n",
    "    keras.layers.Dense(32, activation=\"relu\"),\r\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\r\n",
    "])\r\n",
    "\r\n",
    "model.compile(optimizer='adam',\r\n",
    "              loss='sparse_categorical_crossentropy',\r\n",
    "              metrics=['accuracy'])\r\n",
    "              \r\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "train_images = np.expand_dims(train_images, axis=-1) \r\n",
    "print(train_images.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 1.8314 - accuracy: 0.3469\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 1.5429 - accuracy: 0.4619\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 1.4434 - accuracy: 0.4989\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 1.3818 - accuracy: 0.5193\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 1.3384 - accuracy: 0.5336\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 1.3060 - accuracy: 0.5476\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 1.2784 - accuracy: 0.5560\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 1.2510 - accuracy: 0.5647\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 1.2328 - accuracy: 0.5705\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 1.2120 - accuracy: 0.5787\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x227b8d5da48>"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training the model\r\n",
    "model.fit(train_images, train_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_images = rgb2gray(test_images) \r\n",
    "test_images = np.expand_dims(test_images, axis=-1) \r\n",
    "\r\n",
    "test_images.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 2.3411 - accuracy: 0.0985\n",
      "\n",
      "Test accuracy: 0.09849999845027924\n"
     ]
    }
   ],
   "source": [
    "# Accuracy \r\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose=2)\r\n",
    "\r\n",
    "print('\\nTest accuracy:', test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Una CNN con:\r\n",
    "    - 1 capa convolutiva con 8 neuronas\r\n",
    "    - 1 MaxPool quedando las dimensiones de la imagen a la mitad\r\n",
    "    - 1 dropout 0.25\r\n",
    "    - 1 Flatten\r\n",
    "    - 1 dense con 16 neuronas\r\n",
    "    - 1 dense con 32 neuronas\r\n",
    "    - 1 dense con 10 (salida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 30, 30, 8)         80        \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 15, 15, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 15, 15, 8)         0         \n",
      "_________________________________________________________________\n",
      "flatten_6 (Flatten)          (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 16)                28816     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 32)                544       \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 29,770\n",
      "Trainable params: 29,770\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model 2\r\n",
    "model_2 = keras.Sequential([\r\n",
    "    keras.layers.Conv2D(filters=8, \r\n",
    "                        kernel_size=(3, 3), \r\n",
    "                        input_shape=(32, 32, 1), \r\n",
    "                        padding=\"valid\"),\r\n",
    "    keras.layers.MaxPooling2D(2, 2),\r\n",
    "    keras.layers.Dropout(0.25),\r\n",
    "    keras.layers.Flatten(),\r\n",
    "    keras.layers.Dense(16, activation=\"relu\"),\r\n",
    "    keras.layers.Dense(32, activation=\"relu\"),\r\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\r\n",
    "])\r\n",
    "\r\n",
    "model_2.compile(optimizer='adam',\r\n",
    "              loss='sparse_categorical_crossentropy',\r\n",
    "              metrics=['accuracy'])\r\n",
    "              \r\n",
    "model_2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 1.8666 - accuracy: 0.3231\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 1.6120 - accuracy: 0.4261\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 1.5085 - accuracy: 0.4656\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4548 - accuracy: 0.4871\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.4152 - accuracy: 0.5005\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 1.3891 - accuracy: 0.5123\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 1.3630 - accuracy: 0.5207\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 4s 3ms/step - loss: 1.3501 - accuracy: 0.5275\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3295 - accuracy: 0.5363\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 5s 3ms/step - loss: 1.3171 - accuracy: 0.5399\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x227b90770c8>"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_2.fit(train_images, train_labels, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 0s - loss: 1.3425 - accuracy: 0.5336\n",
      "\n",
      "Test accuracy: 0.5335999727249146\n"
     ]
    }
   ],
   "source": [
    "# Accuracy \r\n",
    "test_loss_2, test_acc_2 = model_2.evaluate(test_images, test_labels, verbose=2)\r\n",
    "\r\n",
    "print('\\nTest accuracy:', test_acc_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first model gives a lost of 2.3411097526550293 and the second 1.3425198793411255. The first model gives an accuracy of 0.09849999845027924 and the second 0.5335999727249146. The first model is more accurate, so an extra dense layer is not required.\n"
     ]
    }
   ],
   "source": [
    "print(f\"The first model gives a lost of {test_loss} and the second {test_loss_2}. The first model gives an accuracy of {test_acc} and the second {test_acc_2}. The first model is more accurate, so an extra dense layer is not required.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.\n",
    "\n",
    "Entrena la red neuronal 1 pero esta vez con las imágenes a color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images_3, train_labels_3), (test_images_3, test_labels_3) = data.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50000, 32, 32, 3)"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images_3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization\r\n",
    "train_images_3 = train_images_3 / 255.0\r\n",
    "test_images_3 = test_images_3 / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_14 (Conv2D)           (None, 30, 30, 8)         224       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_14 (MaxPooling (None, 15, 15, 8)         0         \n",
      "_________________________________________________________________\n",
      "dropout_14 (Dropout)         (None, 15, 15, 8)         0         \n",
      "_________________________________________________________________\n",
      "flatten_14 (Flatten)         (None, 1800)              0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 32)                57632     \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 10)                330       \n",
      "=================================================================\n",
      "Total params: 58,186\n",
      "Trainable params: 58,186\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# model 3\r\n",
    "model_3 = keras.Sequential([\r\n",
    "    keras.layers.Conv2D(filters=8, \r\n",
    "                        kernel_size=(3, 3), \r\n",
    "                        input_shape=(32, 32, 3), \r\n",
    "                        padding=\"valid\"),\r\n",
    "    keras.layers.MaxPooling2D(2, 2),\r\n",
    "    keras.layers.Dropout(0.25),\r\n",
    "    keras.layers.Flatten(),\r\n",
    "    keras.layers.Dense(32, activation=\"relu\"),\r\n",
    "    keras.layers.Dense(10, activation=\"softmax\")\r\n",
    "])\r\n",
    "\r\n",
    "model_3.compile(optimizer='adam',\r\n",
    "              loss='sparse_categorical_crossentropy',\r\n",
    "              metrics=['accuracy'])\r\n",
    "              \r\n",
    "model_3.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.6263 - accuracy: 0.4261\n",
      "Epoch 2/10\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4040 - accuracy: 0.5073\n",
      "Epoch 3/10\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2973 - accuracy: 0.5458\n",
      "Epoch 4/10\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2333 - accuracy: 0.5669\n",
      "Epoch 5/10\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1848 - accuracy: 0.5848\n",
      "Epoch 6/10\n",
      "1563/1563 [==============================] - 6s 4ms/step - loss: 1.1519 - accuracy: 0.5976\n",
      "Epoch 7/10\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.1219 - accuracy: 0.6037\n",
      "Epoch 8/10\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.0990 - accuracy: 0.6161\n",
      "Epoch 9/10\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.0798 - accuracy: 0.6217\n",
      "Epoch 10/10\n",
      "1563/1563 [==============================] - 7s 4ms/step - loss: 1.0626 - accuracy: 0.6266\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x227b92f3e88>"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_3.fit(train_images_3, train_labels_3, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 - 1s - loss: 1.1572 - accuracy: 0.5914\n",
      "\n",
      "Test accuracy: 0.5914000272750854\n"
     ]
    }
   ],
   "source": [
    "# Accuracy \r\n",
    "test_loss_3, test_acc_3 = model_3.evaluate(test_images_3, test_labels_3, verbose=2)\r\n",
    "\r\n",
    "print('\\nTest accuracy:', test_acc_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d8a740277f67c33143a8e5c8e55f738530a350d8def4a85d8635b690074994c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}