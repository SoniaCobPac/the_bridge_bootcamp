{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. En primer lugar se le da un valor numérico diferente a cada palabra o símbolo\n",
    "2. Cuando se ha dado un valor numérico a cada palabra o símbolo (puede ser un array one-hot), se pasa a un embedding (múltiples formas). En este caso, vamos a ver CBOW y Skip-Gram\n",
    "3. La red neuronal va a otorgar una serie de pesos a la palabra, que representa la codificación (vector) de la palabra. \n",
    "4. A partir de ese vector, se puede medir la \"distancia\" que existe entre cada palabra.\n",
    "5. El entrenamiento trata de ajustar los pesos para representar mejor el vector de cada palabra.\n",
    "6. https://projector.tensorflow.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](to_number.png)\n",
    "\n",
    "A vector o número incremental:\n",
    "\n",
    "perro: 0\n",
    "\n",
    "gato: 1\n",
    "\n",
    "mi: 2\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trabaja con una ventana. N-GRAM\n",
    "\n",
    "![](window_vector.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "![](CBOW_Skip_gram.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Usuario\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Python program to generate word vectors using Word2Vec \r\n",
    "\r\n",
    "# importing all necessary modules \r\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \r\n",
    "import warnings \r\n",
    "import nltk\r\n",
    "\r\n",
    "nltk.download('punkt')\r\n",
    "\r\n",
    "warnings.filterwarnings(action = 'ignore') \r\n",
    "\r\n",
    "import gensim \r\n",
    "from gensim.models import Word2Vec \r\n",
    "print()\r\n",
    "\r\n",
    "# pip3 install nltk\r\n",
    "# pip3 install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holooooa\n"
     ]
    }
   ],
   "source": [
    "s = \"Hol\\na\"\n",
    "print(s.replace(\"\\n\", \"oooo\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Reads 'la_ultima_pregunta.txt' file \r\n",
    "sample = open(\"la_ultima_pregunta.txt\", \"r\", encoding=\"utf8\")\r\n",
    "s = sample.read() \r\n",
    "\r\n",
    "# Replaces escape character with space \r\n",
    "f = s.replace(\"\\n\", \" \")    #esto forma parte del pre-procesamiento, no quiero dar distancias al salto de línea\r\n",
    "data = [] \r\n",
    "# Preprocessing\r\n",
    "# iterate through each sentence in the file \r\n",
    "for i in sent_tokenize(f): # tokeniza frases\r\n",
    "    temp = [] \r\n",
    "    print(\"i:\", i)\r\n",
    "    # tokenize the sentence into words \r\n",
    "    for j in word_tokenize(i): # tokeniza palabras\r\n",
    "        temp.append(j.lower()) \r\n",
    "        print(\"j:\", j)\r\n",
    "    data.append(temp)\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CBOW: Cosine similarity between 'última' and 'pregunta': 0.4842565\n",
      "CBOW: Cosine similarity between 'a' and 'con': 0.90121037\n",
      "CBOW: Cosine similarity between ',' and '.': 0.94428027\n",
      "CBOW: Cosine similarity between 'cuando' and '?': 0.6469463\n"
     ]
    }
   ],
   "source": [
    "# Create CBOW model \r\n",
    "model1 = gensim.models.Word2Vec(data, min_count=1, \r\n",
    "                                vector_size=100, #array de 100 números\r\n",
    "                                window = 5, # una palabra tiene en cuenta hasta 5 palabras\r\n",
    "                                sg=False)   \r\n",
    "\r\n",
    "\"\"\"\r\n",
    "size: The number of dimensions of the embeddings and the default is 100.\r\n",
    "\r\n",
    "window: The maximum distance between a target word and words around the target word. The default window is 5.\r\n",
    "\r\n",
    "min_count: The minimum count of words to consider when training the model; words with occurrence less than this count will be ignored. The default for min_count is 5.\r\n",
    "\r\n",
    "workers: The number of partitions during training and the default workers is 3.\r\n",
    "\r\n",
    "sg: The training algorithm, either CBOW(0) or skip gram(1). The default training algorithm is CBOW.\r\n",
    "\"\"\"\r\n",
    "\r\n",
    "def similarity(word1, word2):\r\n",
    "\tprint(\"CBOW: Cosine similarity between '\" + str(word1) + \"' \" + \"and '\" + str(word2) +\r\n",
    "\t\"':\", model1.wv.similarity(str(word1), str(word2)))\r\n",
    "\r\n",
    "similarity('última', 'pregunta')\r\n",
    "similarity('a', 'con')\r\n",
    "similarity(',', '.')\r\n",
    "similarity('cuando', '?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "SKIP_GRAM: Cosine similarity between 'última' and 'pregunta': 0.8821419\n",
      "SKIP_GRAM: Cosine similarity between 'a' and 'con': 0.9752325\n",
      "SKIP_GRAM: Cosine similarity between ',' and '.': 0.9893299\n",
      "SKIP_GRAM: Cosine similarity between 'el' and 'sabes': 0.34929007\n",
      "SKIP_GRAM: Cosine similarity between 'cuando' and '?': 0.8892481\n"
     ]
    }
   ],
   "source": [
    "# Create Skip Gram model \r\n",
    "model2 = gensim.models.Word2Vec(data, min_count = 1, vector_size = 100, \r\n",
    "\t\t\t\t\t\t\t\t\t\t\twindow = 2, sg = 1)\r\n",
    "\r\n",
    "def similarity_2(word1, word2):\r\n",
    "\tprint(\"SKIP_GRAM: Cosine similarity between '\" + str(word1) + \"' \" + \"and '\" + str(word2) +\r\n",
    "\t\"':\", model2.wv.similarity(str(word1), str(word2))) \r\n",
    "\t\r\n",
    "print()\r\n",
    "similarity_2('última', 'pregunta')\r\n",
    "similarity_2('a', 'con')\r\n",
    "similarity_2(',', '.')\r\n",
    "similarity_2('el', 'sabes')\r\n",
    "similarity_2('cuando', '?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model1[\"a\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.02190593,  0.01925903, -0.001875  , -0.00234522,  0.01433094,\n",
       "       -0.00915069,  0.00987992, -0.01795756, -0.00301233,  0.02747907,\n",
       "        0.0088166 , -0.01374047, -0.00321632,  0.01184438,  0.00523952,\n",
       "       -0.00641893, -0.00050455,  0.01143745, -0.00865282,  0.0027733 ,\n",
       "        0.01226304,  0.00337806,  0.0045179 ,  0.01986445, -0.00833196,\n",
       "        0.00254553,  0.02973164, -0.00199891, -0.00870442, -0.00276632,\n",
       "       -0.00726021, -0.00875874, -0.00709109, -0.00204823, -0.00931955,\n",
       "        0.00673598, -0.00494186,  0.00793167,  0.00027097,  0.00683532,\n",
       "        0.01670943, -0.0069272 ,  0.0044862 , -0.00874762, -0.00326368,\n",
       "        0.02777362, -0.00170622,  0.00095657, -0.00909761, -0.01073172,\n",
       "        0.00133445, -0.00160519,  0.0048019 , -0.00174802, -0.01288999,\n",
       "       -0.00247463,  0.00610211,  0.00707935,  0.00718519,  0.00693684,\n",
       "        0.00423035, -0.0012267 , -0.0169231 ,  0.00705289,  0.0103257 ,\n",
       "        0.01457972, -0.0140385 , -0.00829547,  0.01830102, -0.00666804,\n",
       "        0.0177476 , -0.00673493, -0.01352978,  0.01430416, -0.01766617,\n",
       "       -0.00605811, -0.00926497,  0.00795422, -0.00276225, -0.00132279,\n",
       "       -0.00877543, -0.01007619, -0.01231438, -0.0035984 , -0.00479201,\n",
       "       -0.00938808, -0.00920803, -0.00064732, -0.00196712,  0.00835222,\n",
       "        0.00601958, -0.01660291, -0.02307903,  0.0076675 , -0.01386648,\n",
       "        0.00177433, -0.00583369, -0.00532255, -0.00822176,  0.00656844],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_cbow = model1[\"a\"]\n",
    "a_cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.00481788,  0.00453577,  0.00460588,  0.00338535,  0.00034702,\n",
       "       -0.00644896,  0.00174092, -0.00392624,  0.0002319 ,  0.00484577,\n",
       "       -0.00060567, -0.00318063,  0.00349277,  0.00512031,  0.0050165 ,\n",
       "       -0.0020921 ,  0.00016024, -0.00062392, -0.00393995,  0.00311684,\n",
       "        0.00278703,  0.00020568,  0.00159717,  0.00925831, -0.00012311,\n",
       "        0.00200631,  0.00944758,  0.00181495,  0.00138184,  0.00377675,\n",
       "       -0.00483768, -0.00705605, -0.00495305, -0.00049926, -0.00196468,\n",
       "        0.0050768 ,  0.00282952, -0.00172905, -0.00226025,  0.00532068,\n",
       "        0.00971019, -0.00166352,  0.0033332 , -0.00131839, -0.00092696,\n",
       "        0.01156402,  0.0008075 , -0.00463987, -0.00528238, -0.00313133,\n",
       "        0.00264099,  0.00522485,  0.0041494 ,  0.00109488, -0.00979126,\n",
       "        0.00477147,  0.005057  ,  0.00633438,  0.00068203,  0.00054737,\n",
       "        0.00352646,  0.0033446 ,  0.00047429,  0.00476338,  0.00082106,\n",
       "        0.00942463, -0.0028982 , -0.00512662,  0.00438506,  0.00200675,\n",
       "        0.00240322, -0.00497853, -0.00079287,  0.00494361, -0.00450815,\n",
       "       -0.00598834, -0.00022033,  0.00376655, -0.004807  ,  0.00412502,\n",
       "       -0.00402152, -0.00098122, -0.00120332,  0.00246996,  0.00347326,\n",
       "        0.00030555, -0.00224455, -0.00304184,  0.00115199,  0.00308891,\n",
       "        0.00484539, -0.00805531, -0.00852893,  0.00422468, -0.00183442,\n",
       "        0.00040148, -0.0063905 , -0.00393428,  0.00060288, -0.00151331],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuando_cbow = model1[\"cuando\"]\n",
    "cuando_cbow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.58283429e-02,  1.78616010e-02,  1.60965149e-03, -4.70757671e-03,\n",
       "        7.56725855e-03, -5.92489401e-03,  4.50071087e-03, -1.20781166e-02,\n",
       "       -3.97767918e-03,  1.45078301e-02, -5.14111773e-04, -9.72321816e-03,\n",
       "       -6.16474135e-04,  1.05205299e-02,  6.62942650e-03,  1.24115881e-03,\n",
       "        3.08687857e-04,  1.22429747e-02, -1.98204257e-03, -3.32131702e-03,\n",
       "        2.11667572e-03,  1.16970099e-03,  7.12434715e-03,  2.05418691e-02,\n",
       "       -6.09375024e-03,  1.25549140e-03,  2.51561031e-02,  8.74690013e-05,\n",
       "       -1.02225142e-02,  5.98368421e-03, -7.95295555e-03, -7.34242471e-03,\n",
       "       -1.60920981e-03,  1.90290937e-03, -6.27932977e-03,  6.17078179e-03,\n",
       "        1.91950251e-03,  9.82398516e-04, -6.61962479e-03,  9.99939907e-03,\n",
       "        1.52977630e-02, -9.22505977e-04,  2.45108781e-03, -5.86610287e-03,\n",
       "       -1.07429118e-03,  2.38969121e-02, -5.47073036e-03, -2.59175757e-03,\n",
       "       -9.17857885e-03, -6.54999679e-03,  3.72791057e-03, -3.43322195e-03,\n",
       "        9.44936182e-03, -8.52456025e-04, -1.20753851e-02, -4.37894464e-03,\n",
       "        6.26365980e-03,  3.20875505e-03,  7.72294961e-03, -2.58372724e-03,\n",
       "        2.00194423e-03, -3.44629952e-05, -1.39525644e-02,  9.14052688e-03,\n",
       "        4.76747658e-03,  8.62628501e-03, -9.55886953e-03, -7.14302156e-03,\n",
       "        1.41600147e-02, -1.05040474e-02,  1.24643520e-02, -7.58705800e-03,\n",
       "       -1.28314057e-02,  6.37726579e-03, -1.41445072e-02, -8.52064695e-03,\n",
       "       -1.03445854e-02,  8.45584553e-03,  4.70463140e-03,  4.27767821e-03,\n",
       "       -8.80747382e-03, -1.09796105e-02, -9.09999479e-03,  1.58297340e-03,\n",
       "       -2.41088215e-03, -8.32322706e-03, -8.27508513e-03, -6.74369233e-03,\n",
       "        7.09162385e-04,  5.69596794e-03,  5.14322380e-03, -9.88054648e-03,\n",
       "       -1.35660795e-02,  1.37424171e-02, -8.46208725e-03,  3.27460701e-03,\n",
       "       -7.64096761e-03, -2.63346871e-03, -1.72796287e-03,  7.07380241e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_skip_gram = model1[\"con\"]\n",
    "a_skip_gram"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d8a740277f67c33143a8e5c8e55f738530a350d8def4a85d8635b690074994c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}