{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://vgpena.github.io/classifying-tweets-with-keras-and-tensorflow/\n",
    "\n",
    "En el anterior enlace, tenéis un ejemplo sobre cómo, a partir de tweets con un label específico (un sentimiento, positivo o negativo): \n",
    "\n",
    "1. Genera un conjunto de entrenamiento. El conjunto de entrenamiento es formado a partir de tweets completos pasados a un array con un tamaño específico.\n",
    "2. Ese array (X_train de tamaño N) tiene un label que representa el sentimiento (y_train)\n",
    "3. Como todas las frases tienen un tamaño N, la entrada de la red neuronal será de tamaño N y la salida de la red será de tamaño 2 usando activación softmax(porque hay dos clases).\n",
    "\n",
    "Se pide: \n",
    "\n",
    "- Realizar un clasificador de reviews para el dataset de IMDB de la carpeta data_exercise/\n",
    "\n",
    "**Cuando usa la importación \"keras.x\", reemplázalo por \"tensorflow.keras.x\"**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing libreries\r\n",
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import json\r\n",
    "from tensorflow import keras\r\n",
    "import tensorflow.keras.preprocessing.text as kpt\r\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\r\n",
    "from tensorflow.keras.models import Sequential\r\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "training = pd.read_csv(\"data_exercise/IMDB Dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = training[\"review\"]\r\n",
    "y_train = training[\"sentiment\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000,) (50000,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.apply(lambda x: 1 if x == \"positive\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only work with the 3000 most popular words found in the dataset\r\n",
    "max_words = 3000\r\n",
    "\r\n",
    "# create a new Tokenizer\r\n",
    "tokenizer = Tokenizer(num_words=max_words)\r\n",
    "tokenizer.fit_on_texts(X_train)\r\n",
    "\r\n",
    "dictionary = tokenizer.word_index\r\n",
    "\r\n",
    "\r\n",
    "def convert_text_to_index_array(text):\r\n",
    "    # Make all texts the same length, the length of the longest text in the set.\r\n",
    "    return [dictionary[word] for word in kpt.text_to_word_sequence(text)]\r\n",
    "\r\n",
    "allWordIndices = []\r\n",
    "# for each tweet, change each token to its ID in the Tokenizer's word_index\r\n",
    "for text in X_train:\r\n",
    "    wordIndices = convert_text_to_index_array(text)\r\n",
    "    allWordIndices.append(wordIndices)\r\n",
    "\r\n",
    "# list of all tweets and sentiment converted to index arrays.\r\n",
    "allWordIndices = np.asarray(allWordIndices)\r\n",
    "y_train = np.array(y_train)\r\n",
    "\r\n",
    "# create one-hot matrices out of the indexed tweets\r\n",
    "X_train = tokenizer.sequences_to_matrix(allWordIndices, mode='binary')\r\n",
    "# treat the labels as categories\r\n",
    "y_train = keras.utils.to_categorical(y_train, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\r\n",
    "model = Sequential()\r\n",
    "model.add(Dense(512, input_shape=(max_words,), activation='relu'))\r\n",
    "model.add(Dropout(0.5))\r\n",
    "model.add(Dense(256, activation='sigmoid'))\r\n",
    "model.add(Dropout(0.5))\r\n",
    "model.add(Dense(2, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 512)               1536512   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 514       \n",
      "=================================================================\n",
      "Total params: 1,668,354\n",
      "Trainable params: 1,668,354\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\r\n",
    "  optimizer='adam',\r\n",
    "  metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "1407/1407 [==============================] - 10s 7ms/step - loss: 0.3367 - accuracy: 0.8549 - val_loss: 0.2894 - val_accuracy: 0.8782\n",
      "Epoch 2/5\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 0.2459 - accuracy: 0.8973 - val_loss: 0.2731 - val_accuracy: 0.8800\n",
      "Epoch 3/5\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 0.1895 - accuracy: 0.9227 - val_loss: 0.2861 - val_accuracy: 0.8842\n",
      "Epoch 4/5\n",
      "1407/1407 [==============================] - 9s 7ms/step - loss: 0.1260 - accuracy: 0.9494 - val_loss: 0.3661 - val_accuracy: 0.8822\n",
      "Epoch 5/5\n",
      "1407/1407 [==============================] - 9s 6ms/step - loss: 0.0780 - accuracy: 0.9701 - val_loss: 0.4085 - val_accuracy: 0.8770\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1b239ad0548>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training\r\n",
    "model.fit(X_train, y_train,\r\n",
    "  batch_size=32,\r\n",
    "  epochs=5,\r\n",
    "  verbose=1,\r\n",
    "  validation_split=0.1,\r\n",
    "  shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1563/1563 [==============================] - 3s 2ms/step - loss: 0.0688 - accuracy: 0.9768\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.06879564374685287, 0.9767799973487854]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# accuracy\r\n",
    "model.evaluate(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2d8a740277f67c33143a8e5c8e55f738530a350d8def4a85d8635b690074994c"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}